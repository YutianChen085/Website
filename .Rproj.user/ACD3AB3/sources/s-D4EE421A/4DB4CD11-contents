---
title: 'Project 2: Modeling, Testing, and Predicting'
author: "Yutian Chen (yc25997)"
date: '2020-05-13'
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
---

```{r setup, include=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

knitr::opts_chunk$set(echo = TRUE, eval = TRUE,fig.align="center",warning=FALSE,message=FALSE,fig.width=8, fig.height=5, linewidth=60)
options(tibble.width = 100,width = 100)
library(tidyverse)

class_diag<-function(probs,truth){
  
  tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[2,2]/colSums(tab)[2]
  spec=tab[1,1]/colSums(tab)[1]
  ppv=tab[2,2]/rowSums(tab)[2]

  if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1
  
  #CALCULATE EXACT AUC
  ord<-order(probs, decreasing=TRUE)
  probs <- probs[ord]; truth <- truth[ord]
  
  TPR=cumsum(truth)/max(1,sum(truth)) 
  FPR=cumsum(!truth)/max(1,sum(!truth))
  
  dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )

  data.frame(acc,sens,spec,ppv,auc)
}
```

# Modeling

## Instructions

A knitted R Markdown document (as a PDF) and the raw R Markdown file (as .Rmd) should both be submitted to Canvas by 11:59pm on 5/1/2020. These two documents will be graded jointly, so they must be consistent (i.e., donâ€™t change the R Markdown file without also updating the knitted document). Knit an html copy too, for later! In the .Rmd file for Project 2, you can copy the first code-chunk into your project .Rmd file to get better formatting. Notice that you can adjust the opts_chunk$set(...) above to set certain parameters if necessary to make the knitting cleaner (you can globally set the size of all plots, etc). You can copy the set-up chunk in Project2.Rmd: I have gone ahead and set a few for you (such as disabling warnings and package-loading messges when knitting)! 

## Find data:

Find one dataset with at least 5 variables that wish to use to build models. At least one should be categorical (with 2-5 groups) and at least two should be numeric. Ideally, one of your variables will be binary (if not, you will need to create one by discretizing a numeric, which is workable but less than ideal). You will need a minimum of 40 observations (*at least* 10 observations for every explanatory variable you have, ideally 20+ observations/variable).

## Guidelines and Rubric

- **0. (5 pts)** Introduce your dataset and each of your variables (or just your main variables if you have lots) in a paragraph. What are they measuring? How many observations?
```{R}
library(readr)
ICU<-read_csv("ICU.csv")
head(ICU)
```
The Intensive Care Unit Patients dataset, named "ICU", includes data from a sample of 200 pateints at an Intensive Care Unit (ICU). It is a data frame with 200 observations on the 9 variables: patient ID code, patient survival, age (in years), age group, sex, infection status, systolic blood pressure (in mm of Hg), heart rate (beats per minute), and emergency status.

- **1. (15 pts)** 
Perform a MANOVA testing whether any of your numeric variables (or a subset of them, if including them all doesn't make sense) show a mean difference across levels of one of your categorical variables (3). 
If they do, perform univariate ANOVAs to find response(s) showing a mean difference across groups (3), and perform post-hoc t tests to find which groups differ (3). 
Discuss the number of tests you have performed, calculate the probability of at least one type I error (if unadjusted), and adjust the significance level accordingly (bonferroni correction) before discussing significant differences (3). 
Briefly discuss assumptions and whether or not they are likely to have been met (2).
```{R}
#MANOVA
manova<-manova(cbind(Age,SysBP,Pulse)~Survive, data=ICU)
summary(manova)
```
```{R}
#univariate ANOVA
library(dplyr)
summary.aov(manova)
ICU%>%group_by(Survive)%>%summarize(mean(Age),mean(SysBP),mean(Pulse))
#post hoc
pairwise.t.test(ICU$Age,ICU$Survive,p.adj="none")
pairwise.t.test(ICU$SysBP,ICU$Survive,p.adj="none")
```
```{R}
#type I error
#number of tests = 1 MANOVA, 3 ANOVA, 2 post hoc t-tests
1-(0.95^6)
#Boneforroni
0.05/6
```
A MANOVA was conducted to determine the effect of age, systolic blood pressure, and heart rate on patient survival. Significant differences were found between the two patient survival groups for at least one of the dependent variabels (Pillai trace=0.0814, pseudo F(3,196), p(0.0008)<0.05.

Univariate ANOVAs for each dependent variable were conducted as follow-up tests to the MANOVA, using the Bonferroni method for controlling Type I error rates for multiple conparisons. From the ANOVA analysis, we can see that the p-value for test with age was 0.0072 and for test with systolic blood pressure was 0.0036, which were both smllaer than the alpha level of 0.05. Therefore, the univariate ANOVAs for age and systolic blood pressure on patient survival were significant, and we continued with running 2 post-hoc-t-tests. On the other hand, the p-value for test with pulse was 0.6553, which was greater than the alpha level of 0.05. We failed to reject the hypothesis and the effect of pulse on patient survival was not significant; the means of each group were about equal.

We did 1 MANOVAs, 3 ANOVAs and 2 post hoc t-tests. 6 hypothesis tests have been done in total and the probability of at least one type I error was 0.2649. To keep the overall type I error rate at 0.05, the significant level that should be used after Bonferroni correction was 0.0083. After the adjustment, the effects of age and systolic blood pressure were still significant on patient survival groups as their p-values were 0.0072 and 0.0036, accordingly. 

For these hypothesis tests, we assumed the dataset contains random sample and independent observations. These were likely to be met because they are a random sample of 200 patients from a larger study. Multivariate normality of different variables were also likely to be met because each patient survival groups has a sample size larger than 25. Homogeneity of within-group covariance matrices, linear relationships among different vaiables, no extreme univariate or multivariate outliers, and no multicollinearity were assumptions that had not been tested formally and these were uaully hard to be meet. We assumed they were all met when we ran the tests and models.

- **2. (10 pts)** Perform some kind of randomization test on your data (that makes sense). This can be anything you want! State null and alternative hypotheses, perform the test, and interpret the results (7). Create a plot visualizing the null distribution and the test statistic (3).
```{R}
#Randomization test
ICU%>%group_by(Survive)%>%summarize(means=mean(SysBP))%>%summarize(`mean_diff:`=diff(means))
set.seed(400)
rand_bp<-vector()
for (i in 1:5000){
  newdata<-data.frame(bp=sample(ICU$SysBP),survive=ICU$Survive)
  rand_bp[i]<-mean(newdata[newdata$survive=="0",]$bp)-
    mean(newdata[newdata$survive=="1",]$bp)}
mean(rand_bp>16.81875|rand_bp< -16.81875)
#Visualization
{hist(rand_bp,main="Histogram for Mean Systoilc Blood Pressure Difference",ylab="Frequency",xlab="Systolic Blood Pressure (mmHg)",col="pink",breaks=50);
  abline(v=16.81875,col="red");
  abline(v=-16.81875,col="red")}
```
An randomization test was conducted to see whether there was a difference in mean systolic blood pressure (mmHg) between patient who were survivd and who were not survived. A distribution of 5000 mean differences on randomized data was generated. 

The null hypothesis: mean systolic blood pressure (mmHg) was the same for survived vs. not survived patients.
The alternative hypothesis: mean systolic blood pressure (mmHg) was different for sutvived vs. not survived patients.

The resulted p-value of 0.0036, which is smaller than the alpha level of 0.05, indicated that the null hypothesis was rejected and there was a significant difference in mean systolic blood pressure (mmHg) between patient who were survived and who were not survived.There was only 0.36% of mean differences stimulated under the null hypothesis that were more extreme than the actual value of +/-16.82. The visualization plot of the null distribution and the test statistic were shown above.

- **3. (35 pts)** Build a linear regression model predicting one of your response variables from at least 2 other variables, including their interaction. Mean-center any numeric variables involved in the interaction.
    - Interpret the coefficient estimates (do not discuss significance) (10)
    - Plot the regression using `ggplot()`. If your interaction is numeric by numeric, refer to code near the end of WS15 to make the plot. If you have 3 or more predictors, just chose two to plot for convenience. (8)
    - Check assumptions of linearity, normality, and homoskedasticity either graphically or using a hypothesis test (4)
    - Regardless, recompute regression results with robust standard errors via `coeftest(..., vcov=vcovHC(...))`. Discuss significance of results, including any changes from before/after robust SEs if applicable. (8)
    - What proportion of the variation in the outcome does your model explain? (4)
```{R}
library(lmtest)
#Logistic regression
ICU$Age_c<-ICU$Age-mean(ICU$Age,na.rm=T)
ICU$Emergency[ICU$Emergency==1]<-'Yes'
ICU$Emergency[ICU$Emergency==0]<-'No'
fit<-lm(Pulse~Age_c*Emergency,data=ICU)
summary(fit)
#Interaction plot
library(ggplot2)
newICU<-ICU
newICU$Emergency<-rep("Yes",length(newICU$Emergency))
newICU$pred1<-predict(fit,newICU)
newICU$Emergency<-rep("No",length(newICU$Emergency))
newICU$pred2<-predict(fit,newICU)
ggplot(ICU,aes(x=Age_c,y=Pulse,color=Emergency))+
  geom_point()+geom_line(data=newICU,aes(y=pred1),color='lightblue',size=1.5)+
  geom_line(data=newICU,aes(y=pred2),color='pink',size=1.5)
#Kolmogorov-Smirnov test for normality
resids<-fit$residuals
ks.test(resids,"pnorm",mean=0,sd(resids))
#Residuals vs. fitted plots for linearity
fitvals<-fit$fitted.values
ggplot()+geom_point(aes(fitvals,resids))+geom_hline(yintercept=0,color="red")
#Breusch-Pagan test for homoskedasticity
library(sandwich)
bptest(fit)
coeftest(fit)
coeftest(fit,vcov=vcovHC(fit))
#Proportion of variation in the response variable (R square)
sum((fitvals-mean(ICU$Pulse))^2)/sum((ICU$Pulse-mean(ICU$Pulse))^2)
```
A linear regression model was generated to predict patient's pulse from an average age and emergency status: Pulse=94.4-0.54(Age_c)+7.75(Emergency)+0.76(Age_c*Emergency)

Intercept: A patient with an average age and non-emergent status has a predicted pulse of 94.48 bpm.
Age_c: A non-emergent patient show an decrease of 0.54 bpm in pulse for every 1-year increase in age on average. 
EmergencyYes: In patients average age, pulse is 7.75 bpm higher for emergent patients compared to non-emergent patients.
Age_c:EmergencyYes: The slope for age on pulse is 0.76 higher for emergent patient compared to non-emergent patients.

We assumed that the dataset was consist of independent observations and random samples. Assumptions of normality, linearity, and homoskedasticity for this linear regression were checked either graphically or using hypothesis test. A Kolmogorov-Smirnov test was performed to check the normality of the model. A p-value of 0.37 that is greater than the alpha level of 0.05 showed we failed to rejected the null hypothesis. The normality assumption was successfully passed. A residuals versus fitted plots was generated to detect non-linearity, error variances, and outliers of the model. The residuals are scattered randomlly, which means neither linearity nor homoskedasticity were violated. In addition, a Breuch-Pagan test was run to formally test the homoskedasticity assumption. A p-value of 0.53 which is greater than the alpha level of 0.05 showed we failed to rejected the null hypothesis. Thus, the homoskedasticity assumption was met.

Although the homoskedasticity assumption was met, we recompute regression results with robust standard errors. The p-values and standard errors all decreased after robust standard errors were generated, which made the model less conservative. However, there was no big difference before and after robust correction, showing that the robust correction was not necessary as the homoskedasticity assumption was met. 

The proportion of the variation in the outcome was calculated by R square. As the result, the model explaied 0.078 of the variation in the outcome. 

- **4. (5 pts)** Rerun same regression model (with interaction), but this time compute bootstrapped standard errors. Discuss any changes you observe in SEs and p-values using these SEs compared to the original SEs and the robust SEs)
```{R}
#Bootstrapped SEs(resampling rows)
ICU_dat<-ICU[sample(nrow(ICU),replace=TRUE),]
set.seed(500)
samp_ICU<-replicate(5000,{
  boot_ICU<-ICU[sample(nrow(ICU),replace=TRUE),]
  fit<-lm(Pulse~Age_c*Emergency,data=boot_ICU)
  coef(fit)})
samp_ICU%>%t%>%as.data.frame%>%summarize_all(sd)
#Normal-theory SEs
coeftest(fit)
#Robust SEs
coeftest(fit,vcov=vcovHC(fit))
```
We reran the same regression model and computed bootstrapped standard errors. Except the intercept, the standard errors of average age, emergency status and the interaction between age and emergency status were all smaller than normal-theory standard errors and robust standard errors. The p-values in normal-theory standard errors for all variables and interaction were greater than the ones in robust standard errors and bootstrapped standard errors.


- **5. (40 pts)** Perform a logistic regression predicting a binary categorical variable (if you don't have one, make/get one) from at least two explanatory variables (interaction not necessary). 
    - Interpret coefficient estimates in context (10)
    - Report a confusion matrix for your logistic regression (2)
    - Compute and discuss the Accuracy, Sensitivity (TPR), Specificity (TNR), and Recall (PPV) of your model (5)
    - Using ggplot, plot density of log-odds (logit) by your binary outcome variable (3)
    - Generate an ROC curve (plot) and calculate AUC (either manually or with a package); interpret (10)
    - Perform 10-fold (or repeated random sub-sampling) CV and report average out-of-sample Accuracy, Sensitivity, and Recall (10)
```{R}
# Logistic regression
log_ICU<-ICU%>%mutate(y=ifelse(Emergency=="Yes",1,0))
logit<-function(p)log(odds(p))
fitt<-glm(y~Age+Pulse,family="binomial"(link="logit"),data=log_ICU)
coeftest(fitt)
#Confusion matrix
probs<-predict(fitt,type="response")
table(predict=as.numeric(probs>.5),truth=log_ICU$y)%>%addmargins()
#Accuracy
(4+145)/200
#Sensitivity (TPR): the true positive rate
145/147
#Specificity (TNR): the true negative rate
4/53
#Precision (PPV): the proportion calssified emergency who acually are
145/194
#Density plot
log_ICU$logit<-predict(fitt)
log_ICU$outcome<-ifelse(log_ICU$y==1,"Emergent","Not_Emergent")
log_ICU$outcome<-factor(log_ICU$outcome,levels=c("Emergent","Not_Emergent"))
ggplot(log_ICU,aes(logit,fill=outcome))+geom_density(alpha=0.3)+
  geom_vline(xintercept=0,lty=2)+xlab("predictor(logit)")
#ROC Curve and AUC in R
library(plotROC)
ROCplot<-ggplot(data=log_ICU)+geom_roc(aes(d=y,m=probs),n.cuts=0)
ROCplot
calc_auc(ROCplot)
#10-fold CV and Accuracy, Sensitivity, and Precision
set.seed(1234)
k=10
data<-log_ICU%>%sample_frac
folds<-ntile(1:nrow(data),n=10)
diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,]
  test<-data[folds==i,]
  truth<-test$y
  fitt<-glm(y~Age+Pulse,data=train,family="binomial")
  probb<-predict(fitt,newdata=test,type="response")
  diags<-rbind(diags,class_diag(probb,truth))}
summarize_all(diags,mean)
```
A logistic regression model was generated to predict patient's emergency status from age and pulse.

Intercept: odds of emergency for age=0 and pulse=0 is 0.87.
Age: controlling for pulse, for every one additional year-old, odds of emergency decrease by a factor of 0.02.
Pulse: controlling for age, for every one additional beat per minute, odds of emergency increase by a factor of 0.02. 

A confusion matrix was created for the logistic regression. There were 4 true non-emergent patients, 145 true emergent patients, 2 false non-emergent patients, and 49 false emergent patients. From the confusion matrix, accuracy was 0.75, sensitivity was 0.99, specificity was 0.08, and precision was 0.75. 

An receiver operating characteristic (ROC) curve was generated and the area under the curve (AUC) was calculated. The resulted AUC was 0.68, meaning that 68% of randomly selected patient with emergency condition had a higher predicted probability than a randomly selected person without emergency condition. This logisti regression model was not predicting well overall.

A 10-fold cross-validation was performed with the logistic regression model. The average out-of-sample accuracy was 0.725, sensitivity was 0.97, and recall was 0.74. The AUC dropped to 0.66 from 0.68. Although it was not a huge decrease, there were still a little overfitting present.


- **6. (10 pts)** Choose one variable you want to predict (can be one you used from before; either binary or continuous) and run a LASSO regression inputting all the rest of your variables as predictors. 
Choose lambda to give the simplest model whose accuracy is near that of the best (i.e., `lambda.1se`). 
Discuss which variables are retained. Perform 10-fold CV using this model: if response in binary, compare model's out-of-sample accuracy to that of your logistic regression in part 5; if response is numeric, compare the residual standard error (at the bottom of the summary output, aka RMSE): lower is better fit!
```{R}
# LASSO regression
library(glmnet)
set.seed(1234)
y<-as.matrix(ICU$Emergency)
x<-model.matrix(Emergency~.,data=ICU)[,-1][,-1]
x<-scale(x)
cv<-cv.glmnet(x,y,family="binomial")
lasso<-glmnet(x,y,family="binomial",lambda=cv$lambda.1se)
coef(lasso)
# 10-fold CV
lasso_ICU<-ICU%>%mutate(Emergency=ifelse(Emergency=="Yes",1,0))%>%select(ID,Survive,AgeGroup,Sex,Infection,SysBP,Pulse,Emergency)
set.seed(1234)
k=10
data1<-lasso_ICU%>%sample_frac
folds1<-ntile(1:nrow(data1),n=10)
diags1<-NULL
for(i in 1:k){
  train1<-data1[folds1!=i,]
  test1<-data1[folds1==i,]
  truth1<-test1$Emergency
  fittt<-glm(Emergency~.,data=train1,family="binomial")
  probbb<-predict(fittt,newdata=test1,type="response")
  diags1<-rbind(diags1,class_diag(probbb,truth1))}
summarize_all(diags1,mean)
```
After a LASSO regression model was established, "ID"","Survive","AgeGroup","Sex","Infection","SysBP" and "Pulse" were retained to predict the patient's emergency status. A 10-fold cross-validation was performed with the LASSO regression model. The average out-of-sample accuracy was 0.80, sensitivity was 0.92, recall was 0.83 and the AUC was 0.78. Compared to the logistic regression model, the out-of-sample accuracy of the LASSO regression model was higher, indicating that LASSO regression model was less fit.

##### Broader data sources:

[Data.gov](www.data.gov) 186,000+ datasets!

[Social Explorer](Social Explorer) is a nice interface to Census and American Community Survey data (more user-friendly than the government sites). May need to sign up for a free trial.

[U.S. Bureau of Labor Statistics](www.bls.gov)

[U.S. Census Bureau](www.census.gov)

[Gapminder](www.gapminder.org/data), data about the world.

...



